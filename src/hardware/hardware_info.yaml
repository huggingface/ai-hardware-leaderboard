# NVIDIA GPU Configuration
- hardware_type: cuda
  backends:
    vllm:
      docker_args: --runtime nvidia --gpus all
    tgi:
      docker_args: --gpus all --shm-size 64g
    llama_cpp:
      docker_args: --gpus all

# AMD and Intel CPU Configuration
- hardware_type: cpu
  backends:
    vllm:
      docker_args: --device=/dev/cpu:/dev/cpu
    tgi:
      docker_args: --device=/dev/cpu:/dev/cpu
    llama_cpp:
      docker_args: --device=/dev/cpu:/dev/cpu

# Intel Habana Configuration
- hardware_type: habana
  backends:
    # vllm is not compatible with gaudi1
    # vllm:
    #   docker_args: --runtime=habana --device=/dev/habana_pci0:/dev/habana_pci0
    tgi:
      docker_args: --runtime=habana -e PT_HPU_ENABLE_LAZY_COLLECTIVES=true -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none -e ENABLE_HPU_GRAPH=true -e LIMIT_HPU_GRAPH=true -e USE_FLASH_ATTENTION=true -e FLASH_ATTENTION_RECOMPUTE=true --cap-add=sys_nice
      image: ghcr.io/huggingface/tgi-gaudi
    # llama_cpp:
    #   docker_args: --runtime=habana --device=/dev/habana_pci0:/dev/habana_pci0

# Intel GPU Configuration
- hardware_type: intel_gpu
  backends:
    vllm:
      docker_args: --device=/dev/dri:/dev/dri
    tgi:
      docker_args: --device=/dev/dri:/dev/dri
    llama_cpp:
      docker_args: --device=/dev/dri:/dev/dri

# Google TPU Configuration
- hardware_type: tpu
  backends:
    vllm:
      docker_args: --privileged --device=/dev/accel0:/dev/accel0
    tgi:
      docker_args: --privileged --device=/dev/accel0:/dev/accel0
    llama_cpp:
      docker_args: --privileged --device=/dev/accel0:/dev/accel0

# AWS Inferentia Configuration
- hardware_type: inferentia
  backends:
    vllm:
      docker_args: --device=/dev/neuron0:/dev/neuron0
    tgi:
      docker_args: --device=/dev/neuron0:/dev/neuron0
    llama_cpp:
      docker_args: --device=/dev/neuron0:/dev/neuron0

# AMD GPU Configuration
- hardware_type: rocm
  backends:
    vllm:
      docker_args: --network=host --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --device /dev/kfd --device /dev/dri
      image: rocm/vllm
    tgi:
      docker_args: --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --device=/dev/kfd --device=/dev/dri --group-add video --ipc=host --shm-size 256g --net host
      image_suffix: -rocm
    llama_cpp:
      docker_args: --device /dev/kfd --device /dev/dri
      image_suffix: -rocm


# AMD CPU Configuration
- hardware_type: amd_cpu
  backends:
    vllm:
      docker_args: --device=/dev/cpu:/dev/cpu
    tgi:
      docker_args: --device=/dev/cpu:/dev/cpu
    llama_cpp:
      docker_args: --device=/dev/cpu:/dev/cpu
      runtime_options:
        num_threads: auto

# Apple Silicon Configuration
- hardware_type: apple_silicon
  backends:
    vllm:
      docker_args: --platform linux/arm64
    tgi:
      docker_args: --platform linux/arm64
    llama_cpp:
      docker_args: --platform linux/arm64

# Default Settings (used if no hardware_type is specified)
- hardware_type: default_settings
  backends:
    vllm:
      docker_args: null
    tgi:
      docker_args: null
    llama_cpp:
      docker_args: null